{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d65053c-2ef7-44b0-a4ec-fa3875d50e2e",
   "metadata": {},
   "source": [
    "# Homework 1: Regular languages\n",
    "\n",
    "CS/Ling 581  \n",
    "Spring 2024\n",
    "\n",
    "Solve the following problems using the `pyfoma` finite state library. You can find some basic information about pyfoma in its [documentation](https://github.com/mhulden/pyfoma/blob/main/README.md). You'll especially need to consult the description of its [regular expression metalanguage](https://github.com/mhulden/pyfoma/blob/main/docs/RegularExpressionCompiler.ipynb).\n",
    "\n",
    "To turn in your solution, download your notebook using the `File > Download` menu command and submit the `ipynb` file via canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "644e569c-f51f-4212-8232-39ff0090021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyfoma ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b04573-e0a9-403a-85a7-6c7417b541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfoma import FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9191d1c7-ca51-4860-a0c2-18ca1b06f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "try:\n",
    "    get_ipython()\n",
    "    import ipytest\n",
    "    ipytest.autoconfig()\n",
    "    def init_test():\n",
    "        ipytest.clean()\n",
    "    def run_test():\n",
    "        ipytest.run()\n",
    "except NameError:\n",
    "    def init_test():\n",
    "        pass\n",
    "    def run_test():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674086e-6933-4151-9a54-f534239ff35f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Dates\n",
    "\n",
    "Modify the following regular expression so that it only accepts dates in the form `MM/DD/YYYY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f16b9f8b-f41b-4609-97e8-1d796572c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def check_date(text):\n",
    "    pattern = r\"^(0?[1-9]|1[0-2])/(0?[1-9]|[12][0-9]|3[01])/\\d{4}$\"\n",
    "    return re.match(pattern, text) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e5826a-d6f9-45f2-88e0-bf72f5d6351b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_date('2/7/2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a14be30a-b91f-4587-8059-97d1e3e52479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_date('05/0888/200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072ee94-f244-4747-bb74-3c37a9aaa8e6",
   "metadata": {},
   "source": [
    "The following code block will test your regular expression by trying it against a bunch of strings. If your answer is correct it should pass 100% of the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05deeed-16fd-491c-ad9c-6e8f4a0e4fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                   [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "init_test()\n",
    "\n",
    "YES_DATES = ('2/3/2023', '12/14/1999', '11/4/2020', '02/03/2000')\n",
    "NO_DATES = (' 2/3/2023', '22/1/2023', '5/89/1874', '2/1/20230', '2/1/2023/', '2/1/1')\n",
    "\n",
    "@pytest.mark.parametrize('text,isDate', [(x,True) for x in YES_DATES] + \n",
    "                                        [(x,False) for x in NO_DATES])\n",
    "def test_date(text, isDate):\n",
    "    assert check_date(text) == isDate\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0be4f-1be8-4cfd-8caa-a82ac8c137a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2: Numbers\n",
    "\n",
    "Create a transducer that maps the integers 1â€“99 to English. For example, it should map \"12\" to \"twelve\" and \"46\" to \"forty six\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca667a34-50d6-4dd5-b3e2-ac1473472381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forty six']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_defs = dict()\n",
    "\n",
    "# one digit numbers\n",
    "number_defs['s1'] = FST.re(r\"1:(one) | 2: (two) | 3: (three) | 4: (four) | (5): (five) | 6:(six) | (7): (seven) | (8): (eight) | 9: (nine)\", number_defs)\n",
    "\n",
    "# irregular teens with special cases\n",
    "number_defs['s2'] = FST.re(r\"(10):(ten) | (11):(eleven) | (12): (twelve) | (13): (thirteen) | (18): (eighteen)\", number_defs)\n",
    "\n",
    "# regular teens\n",
    "number_defs['s3'] = FST.re(r\"(1[45679]) @ (1:'' $s1 '':(teen))\", number_defs)\n",
    "\n",
    "# double digits ending in 0\n",
    "number_defs['s4'] = FST.re(r\"(2): (twenty) (0:''|'':' '$s1) | (3): (thirty) (0:''|'':' '$s1) | (4): (forty) (0:''|'':' '$s1) | (5): (fifty) (0:''|'':' '$s1) | (6): (sixty) (0:''|'':' '$s1) | (7): (seventy) (0:''|'':' '$s1) | (8): (eighty) (0:''|'':' '$s1) | (9): (ninety) (0:''|'':' '$s1)\" , number_defs)\n",
    "\n",
    "\n",
    "number = FST.re(r\"$s1 | $s2 | $s3 | $s4\" , number_defs)\n",
    "\n",
    "#tester\n",
    "list(number.generate('46'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d86094f-1dbd-4fd3-a9c0-17726ba4ffba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['six']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(number.generate('6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04a7501a-fca7-41c6-8ff3-d9527ed1da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(number.analyze('sixteen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deac63e-0e1e-4b62-85de-12ecd061cdc5",
   "metadata": {},
   "source": [
    "Check your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfaba6ff-c896-4d86-a515-bfb35bc7c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 92%]\n",
      "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m99 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "init_test()\n",
    "\n",
    "PAIRS = [('1','one'),('2','two'),('3','three'),('4','four'),('5','five'),\n",
    "    ('6','six'),('7','seven'),('8','eight'),('9','nine'),('10','ten'),\n",
    "    ('11','eleven'),('12','twelve'),('13','thirteen'),('14','fourteen'),\n",
    "    ('15','fiveteen'),('16','sixteen'),('17','seventeen'),('18','eighteen'),\n",
    "    ('19','nineteen'),('20','twenty'),('21','twenty one'),('22','twenty two'),\n",
    "    ('23','twenty three'),('24','twenty four'),('25','twenty five'),\n",
    "    ('26','twenty six'),('27','twenty seven'),('28','twenty eight'),\n",
    "    ('29','twenty nine'),('30','thirty'),('31','thirty one'),('32','thirty two'),\n",
    "    ('33','thirty three'),('34','thirty four'),('35','thirty five'),\n",
    "    ('36','thirty six'),('37','thirty seven'),('38','thirty eight'),\n",
    "    ('39','thirty nine'),('40','forty'),('41','forty one'),('42','forty two'),\n",
    "    ('43','forty three'),('44','forty four'),('45','forty five'),\n",
    "    ('46','forty six'),('47','forty seven'),('48','forty eight'),\n",
    "    ('49','forty nine'),('50','fifty'),('51','fifty one'),('52','fifty two'),\n",
    "    ('53','fifty three'),('54','fifty four'),('55','fifty five'),\n",
    "    ('56','fifty six'),('57','fifty seven'),('58','fifty eight'),\n",
    "    ('59','fifty nine'),('60','sixty'),('61','sixty one'),('62','sixty two'),\n",
    "    ('63','sixty three'),('64','sixty four'),('65','sixty five'),\n",
    "    ('66','sixty six'),('67','sixty seven'),('68','sixty eight'),\n",
    "    ('69','sixty nine'),('70','seventy'),('71','seventy one'),\n",
    "    ('72','seventy two'),('73','seventy three'),('74','seventy four'),\n",
    "    ('75','seventy five'),('76','seventy six'),('77','seventy seven'),\n",
    "    ('78','seventy eight'),('79','seventy nine'),('80','eighty'),\n",
    "    ('81','eighty one'),('82','eighty two'),('83','eighty three'),\n",
    "    ('84','eighty four'),('85','eighty five'),('86','eighty six'),\n",
    "    ('87','eighty seven'),('88','eighty eight'),('89','eighty nine'),\n",
    "    ('90','ninety'),('91','ninety one'),('92','ninety two'),\n",
    "    ('93','ninety three'),('94','ninety four'),('95','ninety five'),\n",
    "    ('96','ninety six'),('97','ninety seven'),('98','ninety eight'),\n",
    "    ('99','ninety nine')]\n",
    "\n",
    "@pytest.mark.parametrize('digits,text', PAIRS)\n",
    "def test_number(digits,text):\n",
    "\n",
    "    assert list(number.generate(digits)) == [text]\n",
    "    assert list(number.analyze(text)) == [digits]\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a07dde-3452-4467-988a-f5c6519afb74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 3: Tokenization\n",
    "\n",
    "The following cell defines a regular expression for a simple tokenizer. As written, it divides tokens up at spaces with two exceptions: punctuation marks `,!?` are tokens by themselves and the contraction `n't` is treated as a separate token. \n",
    "\n",
    "Modify this expression so that it meets the following additional requirements:\n",
    "- the punctuation marks ` `` `  and `''` (left double apostrophe and right double apostrophe) should be single tokens\n",
    "- like `n't`Â , the contractions `'ve`, `'ll`, `'re`, and `'s` should be seperate tokens\n",
    "- numbers should be separate tokens, where:\n",
    "    - a number may start with `$` or end with `%`\n",
    "    - a number may start with or contain a comma but may not end with one (technically, number tokens shouldn't start with a comma but it's okay if your transducer allows it)\n",
    "\n",
    "Note that English tokenizers also usually have to worry about periods (`.`), which can be used to mark abbreviations, as decimal points, or to end a sentence (among other things). Unfortunately pyfoma has some weird bugs in the way in handles periods, so we'll just ignore them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54d69451-76d9-499c-bf6a-d4848d07b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$120', ',', '000']\n"
     ]
    }
   ],
   "source": [
    "tok_patterns = {}\n",
    "# is it not possible to place a lookbehind assertion for commas that are only near alphabet?\n",
    "# insert spaces before and after punctuation\n",
    "tok_patterns['punct'] = FST.re(r\"$^rewrite('':' ' [!,?-] '':' ')\") \n",
    "    \n",
    "\n",
    "# insert space before n't                      n't        've      'll      're      's\n",
    "tok_patterns['contract'] = FST.re(r\"$^rewrite(('':' 'n\\'t) | ('':' '\\'ve) | ('':' '\\'ll) | ('':' '\\'re) | ('':' '\\'s))\")\n",
    "\n",
    "# number that starts with $ and ends with % [Needs comma with money amount]\n",
    "tok_patterns['money'] = FST.re(r\"$^rewrite('':' '^\\$'':' '[0-9]+)\")\n",
    "\n",
    "tok_patterns['percentages'] = FST.re(r\"$^rewrite('':''[0-9]+\\% '':' ')\")\n",
    "\n",
    "# tokenizes parentheses 8 passes with and without interruptions\n",
    "tok_patterns['parentheses'] = FST.re(r\"$^rewrite('':' '\\`'':' '[a-z]+' ':' '\\`'':' ' |'':' '\\''':' '[a-z]+'':' '\\''':' '|'':'' [``] '':' '[a-zA-Z]+'':' '[a-zA-Z]+'':' '\\'\\''':' ')\")\n",
    "\n",
    "\n",
    "tokenizer = FST.re(\"$punct @ $contract @ $money @ $percentages @ $parentheses\", tok_patterns)\n",
    "\n",
    "# How to declare characters on their own? [``]? \n",
    "# \"The word 'very' is very over-used\"\n",
    "# tok_patterns['parentheses'] = FST.re(r\"$^rewrite('':' '\\`'':' '[a-z]+' ':' '\\`'':' ' |'':' '\\''':' '[a-z]+'':' '\\''':' ')\")\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    s = list(tokenizer.generate(s))\n",
    "    if len(s) == 1:\n",
    "        return s[0].split()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "#tester\n",
    "print(tokenize(\"$120,000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "252285dd-1e78-429f-b77b-abc05af5a4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'you', 'love', 'finite', 'state', 'transducers', '?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Don't you love finite state transducers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a958b9-0b72-4a77-b197-cb1d4ad5fb80",
   "metadata": {},
   "source": [
    "If your regular expression is right, all the tests defined below should pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "511418bf-c627-411d-a544-b0d0760997a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                   [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m____________ test_tokenizer[Is it legal to shout ``Fire!'' in a crowded theater?-toks4] ____________\u001b[0m\n",
      "\n",
      "text = \"Is it legal to shout ``Fire!'' in a crowded theater?\"\n",
      "toks = ['Is', 'it', 'legal', 'to', 'shout', '``', ...]\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m'\u001b[39;49;00m\u001b[33mtext,toks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, TEST_EXAMPLES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tokenizer\u001b[39;49;00m(text, toks):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(text) == toks\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert ['Is', 'it', ...'``Fire', ...] == ['Is', 'it', ...t', '``', ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 5 diff: \u001b[0m\u001b[33m'\u001b[39;49;00m\u001b[33m``Fire\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m != \u001b[0m\u001b[33m'\u001b[39;49;00m\u001b[33m``\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains one more item: \u001b[0m\u001b[33m'\u001b[39;49;00m\u001b[33m?\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/tmp/ipykernel_129/2713797998.py\u001b[0m:24: AssertionError\n",
      "\u001b[31m\u001b[1m___________ test_tokenizer[They're going to pay us 10% of $120,000 by Jun 4, 2021-toks9] ___________\u001b[0m\n",
      "\n",
      "text = \"They're going to pay us 10% of $120,000 by Jun 4, 2021\"\n",
      "toks = ['They', \"'re\", 'going', 'to', 'pay', 'us', ...]\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m'\u001b[39;49;00m\u001b[33mtext,toks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, TEST_EXAMPLES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tokenizer\u001b[39;49;00m(text, toks):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(text) == toks\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert ['They', \"'re...y', 'us', ...] == ['They', \"'re...y', 'us', ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 8 diff: \u001b[0m\u001b[33m'\u001b[39;49;00m\u001b[33m$120\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m != \u001b[0m\u001b[33m'\u001b[39;49;00m\u001b[33m$120,000\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains 2 more items, first extra item: \u001b[0m\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/tmp/ipykernel_129/2713797998.py\u001b[0m:24: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_b9bc2959b32d4dea997b6be9387643b4.py::\u001b[1mtest_tokenizer[Is it legal to shout ``Fire!'' in a crowded theater?-toks4]\u001b[0m - AssertionError: assert ['Is', 'it', ...'``Fire', ...] == ['Is', 'it', ...t', '``', ...]\n",
      "\u001b[31mFAILED\u001b[0m t_b9bc2959b32d4dea997b6be9387643b4.py::\u001b[1mtest_tokenizer[They're going to pay us 10% of $120,000 by Jun 4, 2021-toks9]\u001b[0m - assert ['They', \"'re...y', 'us', ...] == ['They', \"'re...y', 'us', ...]\n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m8 passed\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "init_test()\n",
    "\n",
    "TEST_EXAMPLES = (\n",
    "    ('This is a test!', ['This','is','a','test','!']),\n",
    "    ('Is this a test?', ['Is','this','a','test','?']),\n",
    "    (\"I don't think this is a test\", ['I', 'do', \"n't\", 'think', 'this', 'is', 'a', 'test']),\n",
    "    (\"Thá»§y phi cÆ¡ cá»§a tÃ´i lÃ  Ä‘áº§y Ä‘á»§ cá»§a lÆ°Æ¡n\", \n",
    "        ['Thá»§y', 'phi', 'cÆ¡', 'cá»§a', 'tÃ´i', 'lÃ ', 'Ä‘áº§y', 'Ä‘á»§', 'cá»§a', 'lÆ°Æ¡n']),\n",
    "    (\"Is it legal to shout ``Fire!'' in a crowded theater?\", \n",
    "        ['Is', 'it', 'legal', 'to', 'shout', \"``\", 'Fire', '!', \"''\", 'in', 'a', 'crowded', 'theater','?']),\n",
    "    (\"The word 'very' is very over-used\", \n",
    "        ['The', 'word', \"'\", 'very', \"'\", 'is', 'very', 'over', '-', 'used']),\n",
    "    (\"I don't think we'll've been there yet\", \n",
    "        ['I', 'do', \"n't\", 'think', 'we', \"'ll\", \"'ve\", 'been', 'there', 'yet']),\n",
    "    (\"Give me 12 apples, please\", ['Give', 'me', '12', 'apples', ',', 'please']),    \n",
    "    (\"A 20% tip on a $30 tab is 6 dollars\", \n",
    "        ['A', '20%', 'tip', 'on', 'a', '$30', 'tab', 'is', '6', 'dollars']),\n",
    "    (\"They're going to pay us 10% of $120,000 by Jun 4, 2021\",\n",
    "        ['They', \"'re\", 'going', 'to', 'pay', 'us', '10%', 'of', '$120,000', 'by', 'Jun', '4', ',', '2021']),     \n",
    ")\n",
    "\n",
    "@pytest.mark.parametrize('text,toks', TEST_EXAMPLES)\n",
    "def test_tokenizer(text, toks):\n",
    "    assert tokenize(text) == toks\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f04b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf2887-d6ac-4712-9de2-7e6514947c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
