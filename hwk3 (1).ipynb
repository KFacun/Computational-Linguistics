{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Language models\n",
    "\n",
    "CS/Ling 581  \n",
    "Spring 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytest ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:03:34.527385Z",
     "start_time": "2023-04-19T11:03:31.620082Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%precision 4\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "from cytoolz import concat, sliding_window\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:04:49.172418Z",
     "start_time": "2023-04-19T11:04:49.105020Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.autoconfig()\n",
    "\n",
    "    def init_test():\n",
    "        ipytest.clean()\n",
    "\n",
    "    def run_test():\n",
    "        ipytest.run()\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    def init_test():\n",
    "        pass\n",
    "\n",
    "    def run_test():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram language model\n",
    "\n",
    "We'll load some data and use it to train a simple trigram language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:04:53.484493Z",
     "start_time": "2023-04-19T11:04:53.016070Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    return [line.lower().split() for line in gzip.open(filename)]\n",
    "\n",
    "\n",
    "sentences = read_corpus(\"bnc_train.txt.gz\")\n",
    "sentences_train = sentences[:175000]\n",
    "sentences_test = sentences[175000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T12:29:25.233816Z",
     "start_time": "2023-04-19T12:29:25.202205Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class TrigramLM:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        \"\"\"Normalize sentence and add filler tokens <s> and </s>\"\"\"\n",
    "        return [\"<s>\", \"<s>\"] + [w.lower() for w in sentence] + [\"</s>\",\"</s>\"]\n",
    "\n",
    "    def get_unigram_counts(self, train_corpus):\n",
    "        self.unigrams = Counter(concat(train_corpus))\n",
    "\n",
    "    def get_bigram_counts(self, train_corpus):\n",
    "        self.bigrams = Counter(sliding_window(2, concat(train_corpus)))\n",
    "\n",
    "    def get_trigram_counts(self, train_corpus):\n",
    "        self.trigrams = Counter(sliding_window(3, concat(train_corpus)))\n",
    "\n",
    "    def train(self, train_corpus):\n",
    "        \"\"\"Count bigram and unigram frequencies in the training corpus.\"\"\"\n",
    "        train_corpus = [self.preprocess(sentence) for sentence in train_corpus]\n",
    "        self.get_unigram_counts(train_corpus)\n",
    "        self.get_bigram_counts(train_corpus)\n",
    "        self.get_trigram_counts(train_corpus)\n",
    "        self.V = len(self.unigrams)\n",
    "\n",
    "    def log_prob(self, sentence):\n",
    "        \"\"\"Calculate the log_2 probability of a sentence given the model.\"\"\"\n",
    "        p = 0.0\n",
    "        try:\n",
    "            for (w1, w2, w3) in sliding_window(3, self.preprocess(sentence)):\n",
    "                p = (\n",
    "                        p\n",
    "                        + np.log2(self.trigrams[w1, w2, w3] + self.alpha)\n",
    "                        - np.log2(self.bigrams[w1, w2] + self.alpha * self.V)\n",
    "                )\n",
    "            return p\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T12:29:30.238219Z",
     "start_time": "2023-04-19T12:29:26.493339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-50.0718"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = TrigramLM(alpha=1000)\n",
    "lm.train(sentences_train)\n",
    "lm.log_prob('Testing'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Perplexity\n",
    "\n",
    "Define a function that calculates the perplexity of a model on a corpus (= a list of sentences). Use the definition of perplexity given in section 3.2.1 of Jurafsky and Martin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T12:29:30.240890Z",
     "start_time": "2023-04-19T12:29:30.239442Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity(model, sentenceGroup):\n",
    "    sumLogProbability, wordCount = 0, 0\n",
    "\n",
    "    for sentence in sentenceGroup:\n",
    "        tokens = model.preprocess(sentence)\n",
    "        wordCount += len(tokens) - 3\n",
    "\n",
    "        sentenceLogProbability = model.log_prob(sentence)\n",
    "        sumLogProbability += sentenceLogProbability\n",
    "\n",
    "    averageNegativeLogProbability = -sumLogProbability / wordCount\n",
    "\n",
    "    return np.exp2(averageNegativeLogProbability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15418.3348"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = TrigramLM(alpha=0.1)\n",
    "lm.train(sentences_train)\n",
    "perplexity(lm, sentences_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T11:06:00.123838Z",
     "start_time": "2023-04-19T11:05:56.298965Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 4.04s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def my_trigram_lm():\n",
    "    lm = TrigramLM(alpha=0.1)\n",
    "    lm.train(sentences_train)\n",
    "    return lm\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"sentence,logprob\", [(sentences_train[0], -211.8753), (sentences_test[0], -86.1634)]\n",
    ")\n",
    "def test_trigram_logprob(my_trigram_lm, sentence, logprob):\n",
    "    assert my_trigram_lm.log_prob(sentence) == pytest.approx(logprob, rel=1e-3)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"sentences,perplex\",\n",
    "    [(sentences_train[:100], 3773.9770), (sentences_test[:100], 15418.3348)],\n",
    ")\n",
    "def test_trigram_perplexity(my_trigram_lm, sentences, perplex):\n",
    "    assert perplexity(my_trigram_lm, sentences) == pytest.approx(perplex, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2: Smoothing\n",
    "\n",
    "In the definition for `TrigramLM`, `alpha` is the smoothing parameter. What is the best value to use? Try building models with different values for `alpha` and compute their perplexity on both `sentences_train[:500]` and `sentences_test[:500]`. For `alpha` values, try different powers of 10 (e.g., `[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]`). What patterns do you see and what might account for them? What's the best value of `alpha`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the values of the alpha, I tried different values such as 1e-5, 1e-2 and 1e-1 and noticed as alpha increases, then the train perplexity increases while the test perplexity increases and decreases. This pattern might be accounted for by the value of alpha as alpha increases, or the smoothing value increases, then the less predictability the model becomes or the train perplexity increases. For the test perplexity, the value increases and decreases according to the value of alpha and this could be due to specific alpha value being a more accurate fit for the model. An example that could be made would be increasing alpha or increasing smoothing, creating more generalizations with the data and less predicatability with the text. The best value for alpha is 0.01 as it has the train perplexity of 35.692 and a gap of test perplexity of 2620.16. The gap showcases no overfitting created from the data and an accurate fit for smoothing that isn't undersmoothing/oversmoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T13:50:38.188254Z",
     "start_time": "2023-04-19T13:50:02.716618Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "smoothing_params = [100**-exp for exp in range(5, 0, -1)] \n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1e-10, Train Perplexity: 1.7681421648241777, Test Perplexity: 80604.74032165066\n",
      "Alpha: 1e-08, Train Perplexity: 1.7681859970423177, Test Perplexity: 29523.74979813698\n",
      "Alpha: 1e-06, Train Perplexity: 1.7725681034526346, Test Perplexity: 10817.847259898635\n",
      "Alpha: 0.0001, Train Perplexity: 2.201090919491455, Test Perplexity: 4095.120954266784\n",
      "Alpha: 0.01, Train Perplexity: 35.692763570531405, Test Perplexity: 2620.167290134109\n"
     ]
    }
   ],
   "source": [
    "for alpha in smoothing_params:\n",
    "    model = TrigramLM(alpha)\n",
    "    model.train(sentences_train[:500])\n",
    "    train_perplexity = perplexity(model, sentences_train[:500])\n",
    "    test_perplexity = perplexity(model, sentences_test[:500])\n",
    "    results[alpha] = (train_perplexity, test_perplexity)\n",
    "\n",
    "for alpha, (train_perplexity, test_perplexity) in results.items():\n",
    "    print(f\"Alpha: {alpha}, Train Perplexity: {train_perplexity}, Test Perplexity: {test_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Random sampling\n",
    "\n",
    "Write a function that generates a random sentence by sampling from a trigram language model (see sections 3.3 and 3.4 in Jurafsky & Martin).\n",
    "\n",
    "Here's the basic approach you should take: Every sentence will start with the start symbols `<s> <s>`. The language model gives us the conditional probability of each possible word given that context:\n",
    "$$P(w_1|\\texttt{<s> <s>})=\\frac{C(\\texttt{<s> <s> } w_1)+\\alpha}{C(\\texttt{<s> <s>})+\\alpha V}$$\n",
    "Pick word $w_1$ at random by drawing from this distribution. Let's say the word you pick is `disgruntled`. Now the probability of any word being the next word in the sentence is:\n",
    "$$P(w_2|\\texttt{<s> disgruntled})=\\frac{C(\\texttt{<s> disgruntled } w_2)+\\alpha}{C(\\texttt{<s> disgruntled})+\\alpha V}$$\n",
    "Pick word $w_2$ at random by drawing from this new distribution. Keep going like this until you've picked 50 words or the next word is `</s>` (and the sentence is finished), whichever comes first.\n",
    "\n",
    "To do the random picking, use the function `multinomial` defined below. It takes a dictionary mapping words to probabilities and chooses one word at random using the method shown in Figure 3.3 in Jurafsky & Martin. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate(lm):\n",
    "    sentence = ['<s>', '<s>']\n",
    "    while len(sentence) < 52:\n",
    "        context = (sentence[-2], sentence[-1])\n",
    "\n",
    "        probs = {w: lm.log_prob(context + (w,)) for w in lm.unigrams}\n",
    "        totalProb = sum(probs.values())\n",
    "        normalizedProbs = {w: p / totalProb for w, p in probs.items()}\n",
    "\n",
    "        nextWord = random.choices(list(normalizedProbs.keys()), weights=normalizedProbs.values(), k=1)[0]\n",
    "        if nextWord == '</s>':\n",
    "            break\n",
    "        sentence.append(nextWord)\n",
    "\n",
    "    return ' '.join([w.decode('utf-8') if isinstance(w, bytes) else w for w in sentence[2:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=1e-10\n",
      "narrow veto sofa vertical sports galleries classes levels styles owners 1988 controversy half holding normal 's room cotman aquarium across pp. mount blessed edwardian suffolk dampers lost ! solid talks broad bristol strengthening bacteria areas periodicals burmans manufacturer gallons killinghall speak automatic exhibit power jumped road hard infant activators cheltenham\n",
      "alpha=1e-08\n"
     ]
    }
   ],
   "source": [
    "smoothing_params = [100**-exp for exp in range(5, 0, -1)] \n",
    "results = {}\n",
    "for alpha in smoothing_params:\n",
    "    print(f\"alpha={alpha}\")\n",
    "    lm = TrigramLM(alpha=alpha)\n",
    "    lm.train(sentences_train[:500])\n",
    "    print(generate(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try sampling from language models trained using the same data but with different values for `alpha`. What effect does `alpha` have on the sentences you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger value that alpha is, the more smoothing the testing words will recieve. However, the greater the value of alpha will cause cases of over-smoothing where some data will be incoherent. From the example, I used 0.01 and 10000 and found that the higher the value of alpha, the more smoothing while vice versa for the smaller the value of alpha gets. From the text, I found that alpha creates more coherent sentences as alpha reaches a certain value or level of smoothing from the model and test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
