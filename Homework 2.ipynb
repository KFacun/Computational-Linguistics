{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Confusable expressions\n",
    "\n",
    "CS/Ling 581  \n",
    "Spring 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipytest in /opt/conda/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from ipytest) (8.15.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ipytest) (23.1)\n",
      "Requirement already satisfied: pytest>=5.4 in /opt/conda/lib/python3.11/site-packages (from ipytest) (8.0.2)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.11/site-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from pytest>=5.4->ipytest) (1.3.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipytest) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipytest) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipytest) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipytest) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython->ipytest) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "!pip install ipytest\n",
    "import pytest\n",
    "from cytoolz import concat, sliding_window\n",
    "\n",
    "%precision 2\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.autoconfig()\n",
    "\n",
    "    def init_test():\n",
    "        ipytest.clean()\n",
    "\n",
    "    def run_test():\n",
    "        ipytest.run()\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    def init_test():\n",
    "        pass\n",
    "\n",
    "    def run_test():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = [line.split() for line in gzip.open('confuse.txt.gz','rt')]\n",
    "random.shuffle(data)\n",
    "\n",
    "sentences_train = data[:50000]\n",
    "sentences_test = data[50000:55000]\n",
    "\n",
    "conf_set = [\"their\", \"there\", \"they're\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll want to try several similar (but slightly different) approaches, let's start by defining a general `Confuser` class. It  contains the logic for testing a method but doesn't instantiate any particular method. If you're not familiar with Python classes, [this](http://introtopython.org/classes.html) may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Confuser:\n",
    "    def __init__(self, confusable_set):\n",
    "        self.confusable_set = confusable_set\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        return sentence\n",
    "\n",
    "    def train(self, training_corpus):\n",
    "        pass\n",
    "\n",
    "    def guess(self, sentence, position):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval(self, test_corpus):\n",
    "        \"\"\"Run a confusable-guesser over a test corpus and report the accuracy.\"\"\"\n",
    "        total = 0\n",
    "        right = 0\n",
    "        for sentence in test_corpus:\n",
    "            sentence = self.preprocess(sentence)\n",
    "            for i, w in enumerate(sentence):\n",
    "                if w in self.confusable_set:\n",
    "                    guess = self.guess(sentence, i)\n",
    "                    if guess == sentence[i]:\n",
    "                        right += 1\n",
    "                    total += 1\n",
    "        return right / total * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random guessing\n",
    "\n",
    "If we know nothing at all about the problem (other than that we've got three confusable words), the best we can do is to randomly guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RandomConfuser(Confuser):\n",
    "    def guess(self, line, i):\n",
    "        \"\"\"Choose one of the confusable set at random\"\"\"\n",
    "        return random.choice(self.confusable_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are three choices, then we'd expect to guess correctly about 33% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.85"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_confuser = RandomConfuser(conf_set)\n",
    "random_confuser.eval(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default confuser\n",
    "\n",
    "We can definitely do better than random guessing! One piece of information we can get from our training corpus is the relative frequencies of *there, they're,* and *their*. If they aren't all equally common (which they aren't), we can beat random guessing by always going with the most frequent option.\n",
    "\n",
    "This is (almost) the dumbest method possible, but it often gives surprisingly good results. This is in part due to Zipf's Law, which tells us that there are a few very frequent things and lots of infrequent things. This means that the frequencies of confusable items can be very uneven, and the more uneven the probabilities the better we do with a default choice.\n",
    "\n",
    "Since this method is so simple, we can use it to establish a *baseline* score: it's how well we do if we do almost nothing. It gives a sense of how difficult a problem is. Any fancier solutions can be evaluated against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DefaultConfuser(Confuser):\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        \"\"\"Lowercase sentence.\"\"\"\n",
    "        return [w.lower() for w in sentence]\n",
    "\n",
    "    def train(self, train_corpus):\n",
    "        \"\"\"Find the most common member of the confusion set in the training corpus.\"\"\"\n",
    "        train_corpus = [self.preprocess(sentence) for sentence in train_corpus]\n",
    "        target_counts = Counter(\n",
    "            w for w in concat(train_corpus) if w in self.confusable_set\n",
    "        )\n",
    "        self.default_guess = target_counts.most_common()[0][0]\n",
    "\n",
    "    def guess(self, line, i):\n",
    "        \"\"\"Always guess the most frequent member of the confusion set.\"\"\"\n",
    "        return self.default_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, going with a default is better than random guessing, but not by all that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.49"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = DefaultConfuser(conf_set)\n",
    "baseline.train(sentences_train)\n",
    "baseline.eval(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model\n",
    "\n",
    "With the baseline in mind, we can try using a **bigram language model** to solve the confusables problem. Let's say we see the sequence *in their house* and we want to know if that's correct. We can use a language model to compare $P(\\textit{in, their, house})$ to $P(\\textit{in, there, house})$ and $P(\\textit{in, they're, house})$. Whichever sequence has the highest probability is the one with the correct member of the confusion set (we hope!).\n",
    "\n",
    "To start, by the chain rule we have:\n",
    "\n",
    "$$P(\\textit{in, their, house})=P(\\textit{in})\\times P(\\textit{their}|\\textit{in})\\times P(\\textit{house}|\\textit{their,in})$$\n",
    "\n",
    "We can simplify this is two ways. First, since all the sequences we're testing start with *in* we can leave out the $P(in)$ term without changing the relative order of the three sequences. And second, if we assume a first order Markov model (=bigrams) then we only need to consider one word of context. These two changes get us:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\textit{their, house}|\\textit{in})&=P(\\textit{their}|\\textit{in})\\times P(\\textit{house}|\\textit{their})\\\\\n",
    "&=\\frac{C_B(\\textit{in their})}{C_U(\\textit{in})}\\times\\frac{C_B(\\textit{their house})}{C_U(\\textit{their})}\n",
    "\\end{align}$$\n",
    "\n",
    "where $C_B$ and $C_U$ are bigram and unigram frequencies in the training corpus. We repeat this for *there* and *they're* and use the one with the highest probability. If the probability of all three options turns out to be zero, then we'll fall back on the most frequent overall choice, as we did for the default confuser. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BigramConfuser(DefaultConfuser):\n",
    "    def preprocess(self, sentence):\n",
    "        \"\"\"Normalize sentence and add filler tokens <s> and </s>\"\"\"\n",
    "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "\n",
    "    def get_unigram_counts(self, train_corpus):\n",
    "        self.unigrams = Counter(concat(train_corpus))\n",
    "\n",
    "    def get_bigram_counts(self, train_corpus):\n",
    "        self.bigrams = Counter(sliding_window(2, concat(train_corpus)))\n",
    "\n",
    "    def get_default_guess(self):\n",
    "        self.default_guess = max((self.unigrams[w], w) for w in self.confusable_set)[1]\n",
    "\n",
    "    def train(self, train_corpus):\n",
    "        \"\"\"Count bigram and unigram frequencies in the training corpus.\"\"\"\n",
    "        train_corpus = [self.preprocess(sentence) for sentence in train_corpus]\n",
    "        self.get_unigram_counts(train_corpus)\n",
    "        self.get_bigram_counts(train_corpus)\n",
    "        self.get_default_guess()\n",
    "\n",
    "    def guess(self, sentence, i):\n",
    "        \"\"\"Find the guess that maximizes the probability of the sequence\"\"\"\n",
    "        best_p = 0.0\n",
    "        best_guess = self.default_guess\n",
    "        for guess in self.confusable_set:\n",
    "            p = self.prob(guess, sentence, i)\n",
    "            if p > best_p:\n",
    "                best_p = p\n",
    "                best_guess = guess\n",
    "        return best_guess\n",
    "\n",
    "    def prob(self, guess, sentence, i):\n",
    "        \"\"\"Calculate the probability of a guess in context.\"\"\"\n",
    "        try:\n",
    "            p1 = self.bigrams[sentence[i - 1], guess] / self.unigrams[sentence[i - 1]]\n",
    "            p2 = self.bigrams[guess, sentence[i + 1]] / self.unigrams[guess]\n",
    "            return p1 * p2\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigram model performs much better than the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_confuser = BigramConfuser(conf_set)\n",
    "bigram_confuser.train(sentences_train)\n",
    "bigram_confuser.eval(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **error reduction** is a measure of how much better one model is than another. In this case, the bigram model reduces the error rate of the default model by about 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.01"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_error = 1 - baseline.eval(sentences_test)\n",
    "bigram_error = 1 - bigram_confuser.eval(sentences_test)\n",
    "(bigram_error - baseline_error) / baseline_error * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difficulty with implementing language models is that it's hard to know what the \"right\" answer is. A small error in our code might give us a lower accuracy, but how would we know? For testing purposes, it can be helpful to create tiny training and test sets that are small enough that we can do the relevant calculations by hand and then confirm that our code comes up with the same answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                               [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m14 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "DATA = [\n",
    "    \"The jury did not elaborate , but it added that `` there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable '' .\",\n",
    "    \"Despite the warning , there was a unanimous vote to enter a candidate , according to Republicans who attended .\",\n",
    "    \"When the crowd was asked whether it wanted to wait one more term to make the race , it voted no -- and there were no dissents .\",\n",
    "    \"A highway department source said there also is a plan there to issue some $3 million to $4 million worth of rural roads authority bonds for rural road construction work .\",\n",
    "    \"A highway department source said there also is a plan there to issue some $3 million to $4 million worth of rural roads authority bonds for rural road construction work .\",\n",
    "    \"Pelham said Sunday night there was research being done on whether the `` quickie '' vote on the increase can be repealed outright or whether notice would have to first be given that reconsideration of the action would be sought .\",\n",
    "]\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_bigram_confuser():\n",
    "    conf = BigramConfuser([\"their\", \"there\", \"they're\"])\n",
    "    conf.train(s.split() for s in DATA)\n",
    "    return conf\n",
    "\n",
    "\n",
    "def test_bigram_confuser_default(my_bigram_confuser):\n",
    "    assert my_bigram_confuser.default_guess == \"there\"\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gram,count\", [(\"<s>\", 6), (\"the\", 11), (\"to\", 9), (\"pelham\", 1), (\"zippy\", 0)]\n",
    ")\n",
    "def test_bigram_confuser_uni(my_bigram_confuser, gram, count):\n",
    "    assert my_bigram_confuser.unigrams[gram] == count\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gram,count\",\n",
    "    [\n",
    "        ((\"<s>\", \"<s>\"), 0),\n",
    "        ((\"</s>\", \"<s>\"), 5),\n",
    "        ((\"asked\", \"whether\"), 1),\n",
    "        ((\"the\", \"pinhead\"), 0),\n",
    "    ],\n",
    ")\n",
    "def test_bigram_confuser_bi(my_bigram_confuser, gram, count):\n",
    "    assert my_bigram_confuser.bigrams[gram] == count\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"guess,seq,p\",\n",
    "    [\n",
    "        (\"there\", (\"said\", \"X\", \"also\"), 0.1667),\n",
    "        (\"there\", (\"said\", \"X\", \"should\"), 0.0833),\n",
    "        (\"their\", (\"said\", \"X\", \"should\"), 0.0),\n",
    "        (\"their\", (\"before\", \"X\", \"after\"), 0.0),\n",
    "    ],\n",
    ")\n",
    "def test_bigram_confuser_prob(my_bigram_confuser, guess, seq, p):\n",
    "    assert my_bigram_confuser.prob(guess, seq, 1) == pytest.approx(p, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Add-one smoothing\n",
    "\n",
    "We can improve our bigram model by smoothing, and the simplest method for smoothing is to simply add one to every count. This gives us:\n",
    "\n",
    "$$\n",
    "P(\\textit{their, house}|\\textit{in})=\\frac{C_B(\\textit{in their})+1}{C_U(\\textit{in})+V}\\times\\frac{C_B(\\textit{their house})+1}{C_U(\\textit{their})+V}\n",
    "$$\n",
    "\n",
    "where $V$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BigramAddOneConfuser(BigramConfuser):\n",
    "    def prob(self, guess, sentence, i):\n",
    "        vocabulary_size = len(self.unigrams)\n",
    "        try:\n",
    "            prev_token = sentence[i - 1]\n",
    "            next_token = sentence[i + 1]\n",
    "\n",
    "            numerator1 = self.bigrams[(prev_token, guess)] + 1\n",
    "            denominator1 = self.unigrams[prev_token] + vocabulary_size\n",
    "\n",
    "            numerator2 = self.bigrams[(guess, next_token)] + 1\n",
    "            denominator2 = self.unigrams[guess] + vocabulary_size\n",
    "\n",
    "            probab1 = numerator1 / denominator1\n",
    "            probab2 = numerator2 / denominator2\n",
    "\n",
    "            return probab1 * probab2\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.43"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram1_confuser = BigramAddOneConfuser(conf_set)\n",
    "bigram1_confuser.train(sentences_train)\n",
    "bigram1_confuser.eval(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reduction vs. baseline model = 58.13%\n",
      "Error reduction vs. bigram model   = 19.78%\n"
     ]
    }
   ],
   "source": [
    "bigram1_error = 1 - bigram1_confuser.eval(sentences_test)\n",
    "print(\n",
    "    f\"Error reduction vs. baseline model = {(bigram1_error - baseline_error) / baseline_error * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Error reduction vs. bigram model   = {(bigram1_error - bigram_error) / bigram_error * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_bigram1_confuser():\n",
    "    conf = BigramAddOneConfuser([\"their\", \"there\", \"they're\"])\n",
    "    conf.train(s.split() for s in DATA)\n",
    "    return conf\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"guess,seq,p\",\n",
    "    [\n",
    "        (\"there\", (\"said\", \"X\", \"also\"), 0.0008090),\n",
    "        (\"there\", (\"said\", \"X\", \"should\"), 0.0005393),\n",
    "        (\"their\", (\"said\", \"X\", \"should\"), 9.7087e-5),\n",
    "        (\"their\", (\"before\", \"X\", \"after\"), 0.0001),\n",
    "    ],\n",
    ")\n",
    "def test_bigram1_confuser_prob(my_bigram1_confuser, guess, seq, p):\n",
    "    assert my_bigram1_confuser.prob(guess, seq, 1) == pytest.approx(p, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Trigram model\n",
    "\n",
    "We can increase the context sensitivity of our model without making it too much more complicated by moving to a **trigram** model. Using the chain rule again, we have:\n",
    "\n",
    "$$\n",
    "P(\\textit{live, in, their, house, today})=P(\\textit{live})\\times P(\\textit{in}|\\textit{live})\\times \n",
    "P(\\textit{their}|\\textit{in, live})\\times\n",
    "P(\\textit{house}|\\textit{their, in})\\times\n",
    "P(\\textit{today}|\\textit{house, their})\n",
    "$$\n",
    "\n",
    "And as before, we can simplify this to:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\textit{their, house, today}|\\textit{in,live})&=P(\\textit{their}|\\textit{in, live})\\times\n",
    "P(\\textit{house}|\\textit{their, in})\\times\n",
    "P(\\textit{today}|\\textit{house, their})\\\\\n",
    "&=\\frac{C_T(\\textit{live in their})}{C_B(\\textit{live in})}\\times\n",
    "\\frac{C_T(\\textit{in their house})}{C_B(\\textit{in their})}\\times\n",
    "\\frac{C_T(\\textit{their house today})}{C_B(\\textit{their house})}\n",
    "\\end{align}$$\n",
    "\n",
    "where $C_T$ and $C_B$ are trigram and bigram frequencies.\n",
    "\n",
    "For this problem, implement and evaluate a trigram-based confusable solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class TrigramConfuser(DefaultConfuser):\n",
    "    def preprocess(self, sentence):\n",
    "        \"\"\"Normalize sentence and add filler tokens <s> and </s>\"\"\"\n",
    "        return [\"<s>\", \"<s>\"] + [w.lower() for w in sentence] + [\"</s>\", \"</s>\"]\n",
    "\n",
    "    def get_unigram_counts(self, train_corpus):\n",
    "        self.unigrams = Counter(concat(train_corpus))\n",
    "\n",
    "    def get_bigram_counts(self, train_corpus):\n",
    "        self.bigrams = Counter(sliding_window(2, concat(train_corpus)))\n",
    "\n",
    "    def get_trigram_counts(self, train_corpus):\n",
    "        self.trigrams = Counter(sliding_window(3, concat(train_corpus)))\n",
    "\n",
    "    def get_default_guess(self):\n",
    "        self.default_guess = max((self.unigrams[w], w) for w in self.confusable_set)[1]\n",
    "\n",
    "    def train(self, train_corpus):\n",
    "        \"\"\"Count bigram and unigram frequencies in the training corpus\"\"\"\n",
    "        processed_corpus = [self.preprocess(sentence) for sentence in train_corpus]\n",
    "        self.get_unigram_counts(processed_corpus)\n",
    "        self.get_bigram_counts(processed_corpus)\n",
    "        self.get_trigram_counts(processed_corpus)\n",
    "        self.get_default_guess()\n",
    "\n",
    "    def guess(self, sentence, i):\n",
    "        \"\"\"Find the guess that maximizes the probability of the sequence\"\"\"\n",
    "        best_prob = 0.0\n",
    "        best_guess = self.default_guess\n",
    "        for guess_token in self.confusable_set:\n",
    "            probab = self.calc_prob(guess_token, sentence, i)\n",
    "            if probab > best_prob:\n",
    "                best_prob = probab\n",
    "                best_guess = guess_token\n",
    "        return best_guess\n",
    "\n",
    "    def get_word_from_sentence(self, sentence, i):\n",
    "        if i < 0 or i >= len(sentence):\n",
    "            return \"</s>\"\n",
    "        return sentence[i]\n",
    "\n",
    "    def prob(self, guess_token, sentence, i):\n",
    "        \"\"\"Calculate the probability of a guess in context\"\"\"\n",
    "        try:\n",
    "            prev_two_token = self.get_word_from_sentence(sentence, i - 2)\n",
    "            prev_token = self.get_word_from_sentence(sentence, i - 1)\n",
    "            next_token = self.get_word_from_sentence(sentence, i + 1)\n",
    "            next_two_token = self.get_word_from_sentence(sentence, i + 2)\n",
    "            \n",
    "            probab1 = self.trigrams[prev_two_token, prev_token, guess_token] / self.bigrams[prev_two_token, prev_token]\n",
    "            probab2 = self.trigrams[prev_token, guess_token, next_token] / self.bigrams[prev_token, guess_token]\n",
    "            probab3 = self.trigrams[guess_token, next_token, next_two_token] / self.bigrams[guess_token, next_token]\n",
    "            return probab1 * probab2 * probab3\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your model. How does it compare to the baseline and bigram models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrigramConfuser' object has no attribute 'calc_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trigram_confuser \u001b[38;5;241m=\u001b[39m TrigramConfuser(conf_set)\n\u001b[1;32m      2\u001b[0m trigram_confuser\u001b[38;5;241m.\u001b[39mtrain(sentences_train)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrigram_confuser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mConfuser.eval\u001b[0;34m(self, test_corpus)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentence):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfusable_set:\n\u001b[0;32m---> 22\u001b[0m         guess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m guess \u001b[38;5;241m==\u001b[39m sentence[i]:\n\u001b[1;32m     24\u001b[0m             right \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[76], line 31\u001b[0m, in \u001b[0;36mTrigramConfuser.guess\u001b[0;34m(self, sentence, i)\u001b[0m\n\u001b[1;32m     29\u001b[0m best_guess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_guess\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m guess_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfusable_set:\n\u001b[0;32m---> 31\u001b[0m     probab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_prob\u001b[49m(guess_token, sentence, i)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probab \u001b[38;5;241m>\u001b[39m best_prob:\n\u001b[1;32m     33\u001b[0m         best_prob \u001b[38;5;241m=\u001b[39m probab\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrigramConfuser' object has no attribute 'calc_prob'"
     ]
    }
   ],
   "source": [
    "trigram_confuser = TrigramConfuser(conf_set)\n",
    "trigram_confuser.train(sentences_train)\n",
    "trigram_confuser.eval(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m13 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_trigram_confuser():\n",
    "    conf = TrigramConfuser([\"their\", \"there\", \"they're\"])\n",
    "    conf.train(s.split() for s in DATA)\n",
    "    return conf\n",
    "\n",
    "\n",
    "def test_trigram_confuser_default(my_trigram_confuser):\n",
    "    assert my_trigram_confuser.default_guess == \"there\"\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gram,count\",\n",
    "    [\n",
    "        ((\"<s>\", \"<s>\"), 6),\n",
    "        ((\"</s>\", \"<s>\"), 5),\n",
    "        ((\"asked\", \"whether\"), 1),\n",
    "        ((\"the\", \"pinhead\"), 0),\n",
    "    ],\n",
    ")\n",
    "def test_trigram_confuser_bi(my_trigram_confuser, gram, count):\n",
    "    assert my_trigram_confuser.bigrams[gram] == count\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gram,count\",\n",
    "    [\n",
    "        ((\"<s>\", \"<s>\", \"</s>\"), 0),\n",
    "        ((\"</s>\", \"</s>\", \"<s>\"), 5),\n",
    "        ((\"asked\", \"whether\", \"it\"), 1),\n",
    "        ((\"zippy\", \"the\", \"pinhead\"), 0),\n",
    "    ],\n",
    ")\n",
    "def test_trigram_confuser_tri(my_trigram_confuser, gram, count):\n",
    "    assert my_trigram_confuser.trigrams[gram] == count\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"guess,seq,p\",\n",
    "    [\n",
    "        (\"there\", (\"source\", \"said\", \"X\", \"also\", \"is\"), 1.0),\n",
    "        (\"their\", (\"source\", \"said\", \"X\", \"also\", \"is\"), 0.0),\n",
    "        (\"there\", (\"sunday\", \"night\", \"X\", \"was\", \"research\"), 0.5),\n",
    "        (\"their\", (\"sunday\", \"night\", \"X\", \"was\", \"research\"), 0.0),\n",
    "    ],\n",
    ")\n",
    "def test_trigram_confuser_prob(my_trigram_confuser, guess, seq, p):\n",
    "    assert my_trigram_confuser.prob(guess, seq, 2) == pytest.approx(p, rel=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Trigrams + Add one smoothing\n",
    "\n",
    "Finally, implement and evaluate a trigram model using +1 smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramAddOneConfuser(TrigramConfuser):\n",
    "    def prob(self, guess, sentence, i):\n",
    "        V = len(self.unigrams)\n",
    "        try:\n",
    "            prevTwoWord = self.retrieve_word(sentence, i - 2)\n",
    "            prevWord = self.retrieve_word(sentence, i - 1)\n",
    "            nextWord = self.retrieve_word(sentence, i + 1)\n",
    "            nextTwoWord = self.retrieve_word(sentence, i + 2)\n",
    "\n",
    "            numerator1 = self.trigrams[prevTwoWord, prevWord, guess] + 1\n",
    "            denominator1 = self.bigrams[prevTwoWord, prevWord] + V\n",
    "\n",
    "            numerator2 = self.trigrams[prevWord, guess, nextWord] + 1\n",
    "            denominator2 = self.bigrams[prevWord, guess] + V\n",
    "\n",
    "            numerator3 = self.trigrams[guess, nextWord, nextTwoWord] + 1\n",
    "            denominator3 = self.bigrams[guess, nextWord] + V\n",
    "\n",
    "            p1 = numerator1 / denominator1\n",
    "            p2 = numerator2 / denominator2\n",
    "            p3 = numerator3 / denominator3\n",
    "\n",
    "            return p1 * p2 * p3\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0\n",
    "\n",
    "    def calc_prob(self, guessToken, sentence, i):\n",
    "        \"\"\"\n",
    "        Calculate the probability of a guess token being correct at position i in the sentence.\n",
    "        \"\"\"\n",
    "        return self.prob(guessToken, sentence, i)\n",
    "\n",
    "    def retrieve_word(self, sentence, i):\n",
    "        \"\"\"\n",
    "        Retrieve the word at position i in the sentence.\n",
    "        \"\"\"\n",
    "        if i < 0 or i >= len(sentence):\n",
    "            return None\n",
    "        else:\n",
    "            return sentence[i]\n",
    "    \n",
    "    def guess(self, sentence, i):\n",
    "        bestProb = 0\n",
    "        bestGuess = self.default_guess\n",
    "        for guessToken in self.confusable_set:\n",
    "            probab = self.calc_prob(guessToken, sentence, i)\n",
    "            if probab > bestProb:\n",
    "                bestProb = probab\n",
    "                bestGuess = guessToken\n",
    "        return bestGuess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.43"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram1_confuser = TrigramAddOneConfuser(conf_set)\n",
    "trigram1_confuser.train(sentences_train)\n",
    "trigram1_confuser.eval(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_trigram1_confuser():\n",
    "    conf = TrigramAddOneConfuser([\"their\", \"there\", \"they're\"])\n",
    "    conf.train(s.split() for s in DATA)\n",
    "    return conf\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"guess,seq,p\",\n",
    "    [\n",
    "        (\"there\", (\"source\", \"said\", \"X\", \"also\", \"is\"), 2.544e-5),\n",
    "        (\"their\", (\"source\", \"said\", \"X\", \"also\", \"is\"), 9.803e-7),\n",
    "        (\"there\", (\"sunday\", \"night\", \"X\", \"was\", \"research\"), 7.688e-6),\n",
    "        (\"their\", (\"sunday\", \"night\", \"X\", \"was\", \"research\"), 9.900e-7),\n",
    "    ],\n",
    ")\n",
    "def test_trigram1_confuser_prob(my_trigram1_confuser, guess, seq, p):\n",
    "    assert my_trigram1_confuser.prob(guess, seq, 2) == pytest.approx(p, rel=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
